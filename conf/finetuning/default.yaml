random_seed:      42

training_settings:
  save_finetuned_model:   'munin_dpo' # null for no save
  pct_of_datasize:        100
  dpo_keys:               ['PER', 'ORG', 'QUOTES', 'SRC', 'GEN', 'FUN', 'EMP', 'ROL']
  shuffle:                True
  indices:                
    train:              ['0-87', '89-193'] # excluding 88 (many annotation errors)
    val:                ['0-64'] 


# FastLanguageModel: do not alter structure
model:
  model_name:           "load at runtime"
  max_seq_length:       4096
  dtype:                null    # 'None/null' for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs.
  attn_implementation:  "flash_attention_2"
  load_in_4bit:         False   # Use 4bit on-the-fly quantisation. Slower training but less memory.

# FastLanguageModel: do not alter structure
peft:
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  r:                            16     # Adapter rank, choose any number > 0, but suggested 8, 16, 32, 64, 128
  lora_alpha:                   16
  lora_dropout:                 0       # Supports any, but = 0 is optimized
  bias:                         "none"  # Supports any, but = "none" is optimized
  use_gradient_checkpointing:   true
  use_rslora:                   false   # Support rank stabilized LoRA
  loftq_config:                 null    # And LoftQ
  random_state:                 "load at runtime"

# FastLanguageModel: do not alter structure
sfttrainer:
  max_seq_length:               "load at runtime"
  dataset_num_proc:             4
  packing:                      false  # Can make training 5x faster for short sequences.
                                       # Disabled because the datapoints are already long.

# FastLanguageModel: do not alter structure
dpotrainer:
  model_adapter_name:           "train_model"
  ref_adapter_name:             "ref_model"
  max_length:                   "load at runtime"
  max_prompt_length:            "load at runtime"
  beta:                         0.5 # DPO loss. HP of implicity reward. 
                                    # High beta: less divergence from initial policy. 

# FastLanguageModel: do not alter structure
sft_training_args:
  per_device_train_batch_size:  8
  gradient_accumulation_steps:  1
  warmup_steps:                 2
  num_train_epochs:             1
  learning_rate:                0.0002
  weight_decay:                 0.01
  lr_scheduler_type:            "linear"
  optim:                        "adamw_8bit"
  logging_steps:                2
  seed:                         "load at runtime"
  output_dir:                   "outputs"


# FastLanguageModel: do not alter structure
dpo_training_args:
  per_device_train_batch_size:  1
  gradient_accumulation_steps:  8
  warmup_steps:                 4
  num_train_epochs:             1
  learning_rate:                0.0002
  weight_decay:                 0.01
  lr_scheduler_type:            "linear"
  optim:                        "adamw_8bit"
  logging_steps:                2
  seed:                         "load at runtime"
  output_dir:                   "outputs"
  remove_unused_columns:        false